{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLassignment4",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHu7eYQdx3FpA2NUjY7Yl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muski10/Deep-learning/blob/main/DLassignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDwz6D-sqYti"
      },
      "source": [
        "#Reg no - 20MAI0036\n",
        "#Name - Muskan Bajaj\n",
        "#github link -  https://github.com/muski10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4TLMMUZq7q_"
      },
      "source": [
        "#Question1 (AlexNet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9cVfpVCgymC"
      },
      "source": [
        "#importing essential libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cu1rbS0g-fR"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdZmUbukg-kP",
        "outputId": "7b410086-c9b6-4a09-f9c5-511e287bb17d"
      },
      "source": [
        "AlexNet = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(10))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jExgaS1Ig-m9"
      },
      "source": [
        "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O59RCqWg-p0",
        "outputId": "419fb164-b871-41a9-d991-37e82173ee23"
      },
      "source": [
        "#Keras library for CIFAR dataset\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train),(x_test, y_test)=cifar10.load_data()\n",
        "\n",
        "#Train-validation-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
        "\n",
        "\n",
        "#Dimension of the CIFAR10 dataset\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtVz0n4Wg-sc",
        "outputId": "f32aa3ed-cc23-4197-bfe8-b080eb773788"
      },
      "source": [
        "#Onehot Encoding the labels.\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "#Verifying the dimension after one hot encoding\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ursnD1HEh49n",
        "outputId": "bc7f7e23-e085-4718-a621-03bb6b50fdde"
      },
      "source": [
        "#Defining the parameters\n",
        "batch_size= 100\n",
        "epochs=15\n",
        "#Training the model\n",
        "AlexNet.fit(x_train, y_train,\n",
        "batch_size=batch_size,\n",
        "epochs=epochs,\n",
        "verbose=1,\n",
        "validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "350/350 [==============================] - 42s 25ms/step - loss: 1.7850 - accuracy: 0.3579 - val_loss: 1.9282 - val_accuracy: 0.2730\n",
            "Epoch 2/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.3918 - accuracy: 0.5112 - val_loss: 1.7936 - val_accuracy: 0.3905\n",
            "Epoch 3/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.2385 - accuracy: 0.5703 - val_loss: 2.4369 - val_accuracy: 0.2822\n",
            "Epoch 4/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.1468 - accuracy: 0.6045 - val_loss: 1.6279 - val_accuracy: 0.4623\n",
            "Epoch 5/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 1.0314 - accuracy: 0.6496 - val_loss: 1.7893 - val_accuracy: 0.4080\n",
            "Epoch 6/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 0.9339 - accuracy: 0.6850 - val_loss: 1.5430 - val_accuracy: 0.4793\n",
            "Epoch 7/15\n",
            "350/350 [==============================] - 9s 24ms/step - loss: 0.8466 - accuracy: 0.7132 - val_loss: 1.2700 - val_accuracy: 0.5732\n",
            "Epoch 8/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 0.7556 - accuracy: 0.7505 - val_loss: 1.5268 - val_accuracy: 0.5201\n",
            "Epoch 9/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 0.6649 - accuracy: 0.7827 - val_loss: 1.1786 - val_accuracy: 0.6047\n",
            "Epoch 10/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 0.5834 - accuracy: 0.8083 - val_loss: 1.7891 - val_accuracy: 0.4736\n",
            "Epoch 11/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.4963 - accuracy: 0.8395 - val_loss: 1.6177 - val_accuracy: 0.5059\n",
            "Epoch 12/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.4355 - accuracy: 0.8632 - val_loss: 1.5535 - val_accuracy: 0.5393\n",
            "Epoch 13/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.3805 - accuracy: 0.8823 - val_loss: 1.6522 - val_accuracy: 0.5171\n",
            "Epoch 14/15\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.3254 - accuracy: 0.9002 - val_loss: 1.9436 - val_accuracy: 0.4675\n",
            "Epoch 15/15\n",
            "350/350 [==============================] - 8s 24ms/step - loss: 0.2765 - accuracy: 0.9168 - val_loss: 2.1808 - val_accuracy: 0.4447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc983ea5a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2cFNva7iBBH",
        "outputId": "4f7dd010-ef1e-4ead-9a32-52b7b59b1578"
      },
      "source": [
        "AlexNet.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 2.1775 - accuracy: 0.4458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.177471160888672, 0.4458000063896179]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ6DdAPIiBJW"
      },
      "source": [
        "AlexNet.save('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJxrNc3m0cB"
      },
      "source": [
        "TRANSFER LEARNING USING ALEXNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjWSKNGCmYAa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxKMTa4siBNu"
      },
      "source": [
        "nb_train_samples =60000\n",
        "nb_valid_samples =10000\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VlwWtmJihu6",
        "outputId": "69d3c01d-f635-4ff5-92bc-f75c55cba645"
      },
      "source": [
        "(X_train,Y_train), (X_valid, Y_valid) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# expand new axis, channel axis \n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "# we may need 3 channel (instead of 1)\n",
        "X_train = np.repeat(X_train, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_train = X_train.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_train = tf.image.resize(X_train, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "X_valid = np.expand_dims(X_valid, axis=-1)\n",
        "\n",
        "# [optional]: we may need 3 channel (instead of 1)\n",
        "X_valid = np.repeat(X_valid, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_valid = X_valid.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_valid = tf.image.resize(X_valid, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_valid.shape)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "print((X_train.shape,Y_train.shape))\n",
        "print((X_valid.shape,Y_valid.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(TensorShape([60000, 32, 32, 3]), (60000, 10))\n",
            "(TensorShape([10000, 32, 32, 3]), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdR6J6Axih0y"
      },
      "source": [
        "from keras.models import load_model\n",
        "new_model = load_model('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0zmEQJmirDW",
        "outputId": "9057733c-2b3c-45be-8096-8b9c04a20452"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o5BJUDdnMJl",
        "outputId": "eb1b0e64-1e51-4559-aa90-e1671011a37b"
      },
      "source": [
        "new_model.trainable=False\n",
        "model = tf.keras.Sequential([\n",
        "    new_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 10)                25730506  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 25,730,616\n",
            "Trainable params: 110\n",
            "Non-trainable params: 25,730,506\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uNnBmGPnMVa",
        "outputId": "d2ce3487-a844-4d86-c96c-683cd8f7ca7b"
      },
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "batch_size=batch_size,\n",
        "epochs=10,\n",
        "verbose=1,\n",
        "validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "600/600 [==============================] - 7s 10ms/step - loss: 2.3226 - accuracy: 0.0996 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3016 - accuracy: 0.1126 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1108 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1118 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1125 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3006 - accuracy: 0.1171 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1103 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1110 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1128 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.3013 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M6X2gsJirO-"
      },
      "source": [
        "#Question 2 (VGG16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g8KPjY_kEo9",
        "outputId": "c6bde125-592f-414e-e766-7c24e2d513c9"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "class cifar100vgg:\n",
        "    def __init__(self,train=True):\n",
        "        self.num_classes = 100\n",
        "        self.weight_decay = 0.0005\n",
        "        self.x_shape = [32,32,3]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        if train:\n",
        "            self.model = self.train(self.model)\n",
        "        else:\n",
        "            self.model.load_weights('cifar100vgg.h5')\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
        "\n",
        "        model = Sequential()\n",
        "        weight_decay = self.weight_decay\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes))\n",
        "        model.add(Activation('softmax'))\n",
        "        return model\n",
        "\n",
        "\n",
        "    def normalize(self,X_train,X_test):\n",
        "        #this function normalize inputs for zero mean and unit variance\n",
        "        # it is used when training a model.\n",
        "        # Input: training set and test set\n",
        "        # Output: normalized training set and test set according to the trianing set statistics.\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        print(mean)\n",
        "        print(std)\n",
        "        X_train = (X_train-mean)/(std+1e-7)\n",
        "        X_test = (X_test-mean)/(std+1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "    def normalize_production(self,x):\n",
        "        #this function is used to normalize instances in production according to saved training set statistics\n",
        "        # Input: X - a training set\n",
        "        # Output X - a normalized training set according to normalization constants.\n",
        "\n",
        "        #these values produced during first training and are general for the standard cifar10 training set normalization\n",
        "        mean = 121.936\n",
        "        std = 68.389\n",
        "        return (x-mean)/(std+1e-7)\n",
        "\n",
        "    def predict(self,x,normalize=True,batch_size=50):\n",
        "        if normalize:\n",
        "            x = self.normalize_production(x)\n",
        "        return self.model.predict(x,batch_size)\n",
        "\n",
        "    def train(self,model):\n",
        "\n",
        "        #training parameters\n",
        "        batch_size = 128\n",
        "        maxepoches = 15\n",
        "        learning_rate = 0.1\n",
        "        lr_decay = 1e-6\n",
        "        lr_drop = 20\n",
        "\n",
        "        # The data, shuffled and split between train and test sets:\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "        #data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False)  # randomly flip images\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "        #optimization details\n",
        "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        # training process in a for loop with learning rate drop every 25 epoches.\n",
        "\n",
        "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
        "        model.save_weights('cifar100vgg.h5')\n",
        "        return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 100)\n",
        "    y_test = keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "    model = cifar100vgg()\n",
        "\n",
        "    predicted_x = model.predict(x_test)\n",
        "    residuals = (np.argmax(predicted_x,1)!=np.argmax(y_test,1))\n",
        "    loss = sum(residuals)/len(residuals)\n",
        "    print(\"the validation 0/1 loss is: \",loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121.93584\n",
            "68.38902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "390/390 - 32s - loss: 18.5481 - accuracy: 0.0270 - val_loss: 13.9564 - val_accuracy: 0.0255\n",
            "Epoch 2/15\n",
            "390/390 - 29s - loss: 10.8003 - accuracy: 0.0478 - val_loss: 9.1132 - val_accuracy: 0.0233\n",
            "Epoch 3/15\n",
            "390/390 - 29s - loss: 7.1170 - accuracy: 0.0651 - val_loss: 6.4208 - val_accuracy: 0.0458\n",
            "Epoch 4/15\n",
            "390/390 - 29s - loss: 5.4513 - accuracy: 0.0864 - val_loss: 5.1168 - val_accuracy: 0.0721\n",
            "Epoch 5/15\n",
            "390/390 - 29s - loss: 4.6618 - accuracy: 0.1025 - val_loss: 4.4366 - val_accuracy: 0.1031\n",
            "Epoch 6/15\n",
            "390/390 - 29s - loss: 4.2474 - accuracy: 0.1214 - val_loss: 4.1774 - val_accuracy: 0.1381\n",
            "Epoch 7/15\n",
            "390/390 - 29s - loss: 4.0591 - accuracy: 0.1384 - val_loss: 3.9796 - val_accuracy: 0.1548\n",
            "Epoch 8/15\n",
            "390/390 - 29s - loss: 3.9366 - accuracy: 0.1631 - val_loss: 3.8950 - val_accuracy: 0.1747\n",
            "Epoch 9/15\n",
            "390/390 - 29s - loss: 3.8519 - accuracy: 0.1868 - val_loss: 3.7387 - val_accuracy: 0.2201\n",
            "Epoch 10/15\n",
            "390/390 - 29s - loss: 3.7816 - accuracy: 0.2100 - val_loss: 3.7407 - val_accuracy: 0.2288\n",
            "Epoch 11/15\n",
            "390/390 - 29s - loss: 3.7303 - accuracy: 0.2362 - val_loss: 3.6416 - val_accuracy: 0.2658\n",
            "Epoch 12/15\n",
            "390/390 - 29s - loss: 3.7132 - accuracy: 0.2563 - val_loss: 3.6701 - val_accuracy: 0.2667\n",
            "Epoch 13/15\n",
            "390/390 - 29s - loss: 3.7073 - accuracy: 0.2688 - val_loss: 3.5685 - val_accuracy: 0.2988\n",
            "Epoch 14/15\n",
            "390/390 - 29s - loss: 3.7030 - accuracy: 0.2831 - val_loss: 3.5866 - val_accuracy: 0.3065\n",
            "Epoch 15/15\n",
            "390/390 - 29s - loss: 3.7286 - accuracy: 0.2933 - val_loss: 3.6658 - val_accuracy: 0.3203\n",
            "the validation 0/1 loss is:  0.6797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQV02a0PkEyI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}